{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Introduction\n",
    "\n",
    "In this tutorial, we will use Spark Streaming to stream data from a Cloudant database, and process this continously received stream of data using Spark SQL. \n",
    "\n",
    "Our processing pipeline goes through three stages:\n",
    "1. *_changes feed* is streamed from the given Cloudant database using `CloudantReceiver`. `CloudantReceiver` will receive _changes feed of the database, extract individual JSON documents from the feed, and store these documents in Spark's memory for processing by Spark Streaming. \n",
    "2. Spark Streaming will break up this continous stream of documents into batches. Each batch is a separate RDD, and in our case represents a set of documents collected within 10 secs window. This sequence of batches, or sequence of RDDs is what is called a discretized stream or DStream. \n",
    "3. Each RDD of the DStream is processed using Spark SQL.\n",
    "\n",
    "```\n",
    "|                1                  | -> |            2               |    |          3             |\n",
    "|_changes feed ->... doc3 doc2 doc1 | -> |... [doc4 doc3] [doc2 doc1] | -> |... [pBatch2] [pBatch1] |\n",
    "|      CloudantReceiver             | -> | Spark Streaming: DStream   | -> |      Spark SQL         |\n",
    "```\n",
    "\n",
    "In the steps below, we:\n",
    "1. Initialize StreamingContext and DStream\n",
    "2. Define processing of DStream using Spark SQL\n",
    "3. Actually start processing, and stop it after some time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Initializing DStream"
   ]
  }, 
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%AddJar https://github.com/cloudant-labs/spark-cloudant/releases/download/v1.6.4/cloudant-spark-v1.6.4-167.jar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Provide the details of your cloudant account in *properties* map:\n",
    "\n",
    "- *cloudant.host* - the fully qualified account https URL\n",
    "- *cloudant.username* - the Cloudant account username\n",
    "- *cloudant.password* - the Cloudant account password\n",
    "- *database* - database which changes you want to recieve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import org.apache.spark.streaming.{ Seconds, StreamingContext, Time }\n",
    "import org.apache.spark.{ SparkContext, SparkConf}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.rdd.RDD\n",
    "import java.util.concurrent.atomic.AtomicLong\n",
    "\n",
    "import com.cloudant.spark.CloudantReceiver\n",
    "\n",
    "\n",
    "val properties = Map(\n",
    "    \"cloudant.host\"-> \"xxx.cloudant.com\", \n",
    "    \"cloudant.username\"-> \"xxx\",\n",
    "    \"cloudant.password\"-> \"xxx\",\n",
    "    \"database\"-> \"election2016\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Initialize StreamingContext and DStream\n",
    "\n",
    "- Initialize a StreamingContext with a 10 seconds batch size\n",
    "- Create a DStream of database changes using CloudantReceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val ssc = new StreamingContext(sc, Seconds(10))\n",
    "val changesDStream = ssc.receiverStream(new CloudantReceiver(properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define processing of DStreams\n",
    "\n",
    "- Get SQLContext\n",
    "- For every batch:\n",
    "  - Create a dataframe `tweetsDF`\n",
    "  - Create `tweetsDF2` dataframe with fields `gender`, `state`, and `polarity` \n",
    "  - Calculate and display the cumulative count of tweets\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val sqlContext = new SQLContext(sc)\n",
    "import sqlContext.implicits._\n",
    "\n",
    "val curTotalCount = new AtomicLong(0)\n",
    "changesDStream.foreachRDD((rdd: RDD[String], time: Time) => {\n",
    "    println(s\"========= $time =========\")\n",
    "    val rawTweetsDF = sqlContext.read.json(rdd)\n",
    "    \n",
    "    if (!rawTweetsDF.schema.isEmpty) {\n",
    "        rawTweetsDF.show(10)\n",
    "        \n",
    "        val tweetsDF = rawTweetsDF.select($\"cde.author.gender\", \n",
    "                $\"cde.author.location.state\",\n",
    "                $\"cde.content.sentiment.polarity\")\n",
    "        tweetsDF.show(10)\n",
    "        \n",
    "        curTotalCount.getAndAdd(tweetsDF.count())\n",
    "        println(\"Current total count:\" + curTotalCount)\n",
    "    }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Start receiving and processing of data\n",
    "\n",
    "- Start StreamingContext\n",
    "- Allow processing to run for 300 secs\n",
    "- Manually stop processing \n",
    "\n",
    "All previous instructions were just initilizations and definions, and nothing will happen until we start StreamingContext. After the start, the data will be received and processed. Since, DStream is continous,  it will not stop until we manually stop the processing.\n",
    "\n",
    "While this processing is running for 300 secs, we can go back to the Python notebook, and load more data to the database. These new changes will be picked up by Spark Streaming, proccessed and displayed below.\n",
    "Thus, the steps for demonstrating dynamic nature of Spark Streaming are following:\n",
    "\n",
    "1. Run the cell below of the current **Scala notebook**\n",
    "2. After that go back to the **Python notebook**, and run the following two cells of the Python notebook:\n",
    "Run:\n",
    "```python\n",
    "query = \"#election2016\"\n",
    "count = 300\n",
    "```\n",
    "ant then run the following cell:\n",
    "```python\n",
    "TtC = TwitterToCloudant()\n",
    "TtC.count = count\n",
    "TtC.query_twitter(properties, None, query, 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep iterating through the load of tweets into the database and see how the stream below picks up the changes. Every 10 sec we fetch the next set of tweets available at that point. The analysis results will change appropriately and you can see for example different states showing up at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "Thread.sleep(300000L)\n",
    "ssc.stop(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
