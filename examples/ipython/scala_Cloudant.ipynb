{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala example using Spark SQL over Cloudant as a source\n",
    "\n",
    "This sample notebook is written in Scala and expects the Scala 2.10 runtime. Make sure the kernel is started and we are connected when executing this notebook.\n",
    "\n",
    "The data source for this example can be found at: http://examples.cloudant.com/crimes/\n",
    "\n",
    "Replicate the database into your own Cloudant account before you execute this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Work with the Spark Context\n",
    "\n",
    "A Spark Context handle `sc` is available with every notebook create in the Spark Service. Use it to understand the Spark version used, the environment settings, and create a Spark SQL Context object off of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val sqlCtx = new org.apache.spark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Work with a Cloudant database\n",
    "\n",
    "A Dataframe object can be created directly from a Cloudant database. To configure the database as source, pass these options:\n",
    "\n",
    "1 - package name that provides the classes (like `CloudantDataSource`) implemented in the connector to extend `BaseRelation`. For the Cloudant Spark connector this will be `com.cloudant.spark`\n",
    "\n",
    "2 - `cloudant.host` parameter to pass the Cloudant account name\n",
    "\n",
    "3 - `cloudant.user` parameter to pass the Cloudant user name\n",
    "\n",
    "4 - `cloudant.password` parameter to pass the Cloudant account password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use dbName=crimes, indexName=null, jsonstore.rdd.partitions=5, jsonstore.rdd.maxInPartition=-1, jsonstore.rdd.minInPartition=10, jsonstore.rdd.requestTimeout=100000,jsonstore.rdd.concurrentSave=-1,jsonstore.rdd.bulkSize=1\n"
     ]
    }
   ],
   "source": [
    "val df = sqlCtx.read.format(\"com.cloudant.spark\").option(\"cloudant.host\",\"examples.cloudant.com\").option(\"cloudant.username\", \"examples\").option(\"cloudant.password\",\"xxxx\").load(\"crimes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Work with a Dataframe\n",
    "\n",
    "At this point all transformations and functions should behave as specified with Spark SQL. (http://spark.apache.org/sql/)\n",
    "\n",
    "There are, however, a number of things the Cloudant Spark connector does not support yet, or things that are simply not working. For that reason we call this connector a **BETA** release and are only gradually improving it towards GA. \n",
    "\n",
    "Please direct your any change requests at [support@cloudant.com](mailto:support@cloudant.com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: string (nullable = true)\n",
      " |-- _rev: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- compnos: string (nullable = true)\n",
      " |    |-- domestic: boolean (nullable = true)\n",
      " |    |-- fromdate: long (nullable = true)\n",
      " |    |-- main_crimecode: string (nullable = true)\n",
      " |    |-- naturecode: string (nullable = true)\n",
      " |    |-- reptdistrict: string (nullable = true)\n",
      " |    |-- shooting: boolean (nullable = true)\n",
      " |    |-- source: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|naturecode|\n",
      "+----------+\n",
      "|    DISTRB|\n",
      "|       EDP|\n",
      "|    ARREST|\n",
      "|        AB|\n",
      "|      CD14|\n",
      "|    UNKEMS|\n",
      "|      REQP|\n",
      "|       EDP|\n",
      "|       MVA|\n",
      "|     IVPER|\n",
      "|      NIDV|\n",
      "|        AB|\n",
      "|    IVPREM|\n",
      "|     IVPER|\n",
      "|     IVPER|\n",
      "|       MVA|\n",
      "|      CD11|\n",
      "|    LARCEN|\n",
      "|       MVA|\n",
      "|    ARREST|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"properties.naturecode\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "|                 _id|                _rev|            geometry|          properties|   type|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "|79f14b64c57461584...|1-d9518df5c255e4b...|[List(-71.0598744...|[142035012,true,1...|Feature|\n",
      "|79f14b64c57461584...|1-798ef404b141dfb...|[List(-71.0554045...|[142035053,false,...|Feature|\n",
      "|79f14b64c57461584...|1-08cd46894f9c579...|[List(-71.1317963...|[142035113,false,...|Feature|\n",
      "|79f14b64c57461584...|1-be4512491803441...|[List(-71.1331290...|[142035116,false,...|Feature|\n",
      "|79f14b64c57461584...|1-2e3e1fe35278b5d...|[List(-71.0784045...|[142035162,false,...|Feature|\n",
      "|79f14b64c57461584...|1-e03133da93c2644...|[List(-71.0824845...|[142035211,false,...|Feature|\n",
      "|79f14b64c57461584...|1-4c21d07bfb9f45a...|[List(-71.1013666...|[142035316,false,...|Feature|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.col(\"properties.naturecode\").startsWith(\"DISTRB\")).show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Scala 2.10",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  },
  "name": "Cloudant-spark.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}